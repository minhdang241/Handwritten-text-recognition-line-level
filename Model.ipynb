{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                        # root package\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader    # dataset representation and loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_beam_search import WordBeamSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels + filenames\n",
    "chars = set()\n",
    "labels = L()\n",
    "files = L()\n",
    "bad_samples = L()\n",
    "data_root_path = Path(\"../data2\")\n",
    "for line in open(data_root_path/\"sentences.txt\").read().splitlines():\n",
    "    if line[0] == \"#\":\n",
    "        continue\n",
    "    tokens = line.split(\" \")\n",
    "    assert len(tokens) > 9\n",
    "    # Get file names\n",
    "    filename_token = tokens[0].split(\"-\")\n",
    "    file_path = data_root_path/f'{filename_token[0]}/{filename_token[0] + \"-\" + filename_token[1]}/{tokens[0]}.png'  \n",
    "    if not os.path.getsize(file_path):\n",
    "        bad_samples.append(file_path)\n",
    "        continue\n",
    "    label = \" \".join(\" \".join(tokens[9:]).split(\"|\"))\n",
    "    labels.append(label)\n",
    "    chars = chars.union(set(list(label)))\n",
    "    files.append(file_path)\n",
    "    assert len(files) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "charlist = sorted(L(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A MOVE to stop Mr. Gaitskell from\n",
      "nominating any more Labour life Peers\n",
      "Griffiths , M P for Manchester Exchange .\n",
      "A MOVE to stop Mr. Gaitskell from nominating\n",
      "any more Labour life Peers is to be made at a\n",
      "and he is to be backed by Mr. Will Griffiths ,\n",
      "Though they may gather some Left-wing support , a\n",
      "ment Bill which brought life peers into existence , they\n",
      "Delegates from Mr. Kenneth Kaunda's United National Independence\n",
      "We believe that a comprehensive medical service , free to the patient\n",
      "It said that the President and Premier noted \" with satisfaction \" the\n",
      "said the Vienna talks \" might be the beginning of a slight improvement , \"\n",
      "assuredness \" Bella Bella Marie \" ( Parlophone ) , a lively song that changes tempo mid-way .\n"
     ]
    }
   ],
   "source": [
    "maxLength = 0\n",
    "for idx, line in enumerate(labels):\n",
    "    if len(line) > maxLength:\n",
    "        maxLength = len(line)\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(label, max_seq_len=127):\n",
    "    encoded_label = np.zeros(max_seq_len) - 1\n",
    "    i = 0\n",
    "    for word in label:\n",
    "        word = word.replace(\"&quot\", r'\"')\n",
    "        word = word.replace(\"&amp\", r'&')\n",
    "        word = word.replace('\";', '\\\"')\n",
    "        for letter in word:\n",
    "            encoded_label[i] = charlist.index(letter)\n",
    "            i += 1\n",
    "        for idx, val in enumerate(encoded_label):\n",
    "            if val == -1:\n",
    "                encoded_label[idx] = 79\n",
    "    return encoded_label, len(label), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = L()\n",
    "for label in labels:\n",
    "    encoded_labels.append(encode_text(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"fnames\": files,\n",
    "    \"labels\": labels,\n",
    "    \"encoded_labels\": encoded_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, imgSize, dataAugmentation=False):\n",
    "    \"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "    \n",
    "    # there are damaged files in IAM dataset - just use black image instead\n",
    "    if img is None:\n",
    "        img = np.zeros([imgSize[1], imgSize[0]])\n",
    "    \n",
    "    # increase dataset size by applying random stretches to the images\n",
    "    if dataAugmentation:\n",
    "        stretch = (random.random() - 0.5) # -0.5 .. +0.5\n",
    "        wStretched = max(int(img.shape[1] * (1 + stretch)), 1) # random width, but at least 1\n",
    "        img = cv2.resize(img, (wStretched, img.shape[0])) # stretch horizontally by factor 0.5 .. 1.5\n",
    "    \n",
    "    # create target image and copy sample image into it\n",
    "    (wt, ht) = imgSize\n",
    "    (h, w) = img.shape\n",
    "    fx = w / wt\n",
    "    fy = h / ht\n",
    "    f = max(fx, fy)\n",
    "    newSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n",
    "    img = cv2.resize(img, newSize)\n",
    "    target = np.ones([ht, wt]) * 255\n",
    "    target[0:newSize[1], 0:newSize[0]] = img\n",
    "    \n",
    "    # transpose for TF\n",
    "    img = target\n",
    "    \n",
    "    # normalize\n",
    "    #(m, s) = cv2.meanStdDev(img)\n",
    "    #m = m[0][0]\n",
    "    #s = s[0][0]\n",
    "    #img = img - m\n",
    "    #img = img / s if s>0 else img\n",
    "    img = img/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    batchSize = 64\n",
    "    imgSize = (128, 32)\n",
    "    maxTextLen = 32\n",
    "    \n",
    "@Transform\n",
    "def img_tfm(x: PILImage):\n",
    "    data = np.array(x)\n",
    "    result = torch.from_numpy(preprocess(data, (512, 32), True)).float()\n",
    "    return result.unsqueeze(0)\n",
    "\n",
    "def get_x(x): return x['fnames']\n",
    "def get_y(x): return x['encoded_labels']\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock(cls=PILImageBW), None),\n",
    "    get_x = get_x, \n",
    "    get_y = get_y,\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    item_tfms=img_tfm\n",
    ")\n",
    "\n",
    "dset = dblock.datasets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, src, get_x, get_y, item_tfms=None, data_augment=False):\n",
    "        self.item_tfms=item_tfms\n",
    "        self.x = get_x(src)\n",
    "        self.y = get_y(src)\n",
    "        self.data_augment = data_augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_path = self.x[idx]\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if self.item_tfms:\n",
    "            img = self.item_tfms(img, (512, 32), self.data_augment)\n",
    "\n",
    "        return torch.from_numpy(np.expand_dims(img, axis=0)).float(), self.y.values[idx] \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[0 : 16703, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.loc[0:13375, :].reset_index()\n",
    "df_valid = df.loc[13376:, :].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataSet(src=df_train, get_x=get_x, get_y=get_y, item_tfms=preprocess, data_augment=True)\n",
    "valid_set = MyDataSet(src=df_valid, get_x=get_x, get_y=get_y, item_tfms=preprocess, data_augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_set, batch_size=64)\n",
    "valid_dl = DataLoader(valid_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls2 = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3340.8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16704*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(Module):\n",
    "    def __init__(self, vocab_sz, bs):\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "                      \n",
    "        layers.append(nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        layers.append(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm2d(256)) #[64, 256, 8, 128]\n",
    "        \n",
    "        layers.append(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=(2,1), stride=(2, 1))) # [64, 256, 4, 128]\n",
    "        \n",
    "        layers.append(nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm2d(512))# [64, 512, 4, 128]\n",
    "        \n",
    "        layers.append(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))) #[64, 512, 2, 128]\n",
    "        \n",
    "        layers.append(nn.Conv2d(512, 512, kernel_size=2, stride=1)) ##[64, 512, 2, 127]\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        self.cnn = nn.Sequential(*layers)    \n",
    "        self.rnn = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True) \n",
    "        self.h_o = nn.Linear(256*2, vocab_sz + 1)\n",
    "        self.h = [torch.zeros(2*2, bs, 256).cuda() for _ in range(2)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x) #N x C x H x W\n",
    "        out = out.squeeze(2)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        res, h = self.rnn(out, self.h)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self):\n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = str().join(charlist)\n",
    "wordChars = open('../data/wordCharList.txt').read().splitlines()[0]\n",
    "corpus = open('../data/corpus.txt').read()\n",
    "# init decoder object\n",
    "# this step usually only has to be done once in a program\n",
    "beamWidth = 100\n",
    "lmType = 'NGrams'\n",
    "lmSmoothing = 0.01\n",
    "wbs = WordBeamSearch(beamWidth, lmType, lmSmoothing, corpus, chars, wordChars)\n",
    "\n",
    "def ctc_loss_func(xb, yb):\n",
    "    preds = F.log_softmax(xb, dim=2)\n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    pred_lengths = torch.full(size=(64,), fill_value=127, dtype=torch.long)\n",
    "    loss = nn.CTCLoss(blank=79)\n",
    "    targets = yb[0]\n",
    "    target_lengths = yb[1]\n",
    "    return loss(preds, targets, pred_lengths, target_lengths)\n",
    "\n",
    "def my_accuracy(xb, yb):\n",
    "    print(xb.shape)\n",
    "    preds = F.log_softmax(xb, dim=2)\n",
    "    preds = preds.permute(1, 0, 2).cpu().data.numpy()\n",
    "    res = wbs.compute(preds)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, opt, epoch=1):\n",
    "    for i in range(1, epoch+1):\n",
    "        print(\"Training\")\n",
    "        batch_losses = L()\n",
    "        for index, (xb, yb) in enumerate(dls.train):\n",
    "            opt.zero_grad()\n",
    "            print(\"Hello\")\n",
    "            preds = model(xb)\n",
    "            print(preds)\n",
    "            loss = ctc_loss_func(preds, yb)\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            batch_losses.append(loss.item())\n",
    "            print(f\"Training Batch {index}...\")\n",
    "        print(f'Epoch {i}/{epoch} Training Loss: {np.array(batch_losses).mean()} ')    \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            print(\"Validating\")\n",
    "            validate_batch_losses = L()\n",
    "            for xb, yb in dls.validate:\n",
    "                preds = model(xb)\n",
    "                loss = ctc_loss_func(preds, yb)\n",
    "                validate_batch_losses.append(loss.item())\n",
    "                \n",
    "            print(f'Epoch {i}/{epoch} Validation Loss: {np.array(validate_batch_losses).mean()} ')    \n",
    "        model.reset()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls2 = dls2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN(len(charlist), 64).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = dls2.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2c5e4b44d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0][0].cpu().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls2, model, opt_func=SGD, loss_func=ctc_loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.806605</td>\n",
       "      <td>4.328156</td>\n",
       "      <td>04:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.032254</td>\n",
       "      <td>3.934395</td>\n",
       "      <td>04:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.787851</td>\n",
       "      <td>3.724915</td>\n",
       "      <td>04:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.642362</td>\n",
       "      <td>3.592831</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.539836</td>\n",
       "      <td>3.501291</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.470929</td>\n",
       "      <td>3.441061</td>\n",
       "      <td>04:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.421545</td>\n",
       "      <td>3.398032</td>\n",
       "      <td>04:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.386374</td>\n",
       "      <td>3.378510</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.358937</td>\n",
       "      <td>3.340766</td>\n",
       "      <td>04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.338353</td>\n",
       "      <td>3.348453</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2c5dd66690>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0][0].cpu().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(encoded_label):\n",
    "    res = \"\"\n",
    "    for val in encoded_label:\n",
    "        if val >= len(charlist):\n",
    "            continue\n",
    "        res += charlist[int(val)]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It doesn't get it .\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(yb[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35., 72.,  0., 56., 67., 57., 71., 66.,  5., 72.,  0., 59., 57., 72.,  0., 61., 72.,  0., 12., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79.,\n",
       "        79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79.,\n",
       "        79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79.,\n",
       "        79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79., 79.], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([ 6.6567,  3.3153,  2.7651,  2.3725,  2.2075,  2.1808,  2.0791,  2.0687,  2.0608,  1.9657,  1.8980,  1.4804,  1.3798,  0.9676,  0.8225,  0.8057,  0.6704,  0.6537,  0.5671,  0.5510,  0.4103,\n",
       "         0.3926,  0.1763,  0.0741, -0.1171]),\n",
       "indices=tensor([79,  0, 57, 72, 67, 53, 66, 71, 61, 70, 60, 56, 64, 73, 12, 55, 65, 58, 75, 59, 68, 77, 54, 10, 74]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(results[0][0], 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     entertainment'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dls.train_ds\n",
    "valid_ds = dls.valid_ds\n",
    "train_dl = DataLoader(train_ds, batch_size=64)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 127, 80])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = wbs.compute(results.permute(1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy decoder\n",
    "def greedy_decoder(data):\n",
    "\t# index for largest probability each row\n",
    "    \n",
    "\treturn [torch.argmax(s, dim=1) for s in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([127, 80])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = torch.argmax(results[0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(data, k):\n",
    "\tsequences = [[list(), 0.0]]\n",
    "\t# walk over each step in sequence\n",
    "\tfor row in data:\n",
    "\t\tall_candidates = list()\n",
    "\t\t# expand each current candidate\n",
    "\t\tfor i in range(len(sequences)):\n",
    "\t\t\tseq, score = sequences[i]\n",
    "\t\t\tfor j in range(len(row)):\n",
    "\t\t\t\tcandidate = [seq + [j], score - log(row[j])]\n",
    "\t\t\t\tall_candidates.append(candidate)\n",
    "\t\t# order all candidates by score\n",
    "\t\tordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "\t\t# select k best\n",
    "\t\tsequences = ordered[:k]\n",
    "\treturn sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-0c4258fb63f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeam_search_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-774bda40af34>\u001b[0m in \u001b[0;36mbeam_search_decoder\u001b[0;34m(data, k)\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                 \u001b[0mcandidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                                 \u001b[0mall_candidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;31m# order all candidates by score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "beam_search_decoder(results.data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79,\n",
       "        79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79,\n",
       "        79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-19e5fd9cdb5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-3ebe13bdfe7c>\u001b[0m in \u001b[0;36mgreedy_decoder\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgreedy_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# index for largest probability each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-3ebe13bdfe7c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgreedy_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# index for largest probability each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "texts = greedy_decoder(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-520e8cd87ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgreedy_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-8ecadaa3c4b2>\u001b[0m in \u001b[0;36mgreedy_decoder\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgreedy_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# index for largest probability each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-8ecadaa3c4b2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgreedy_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# index for largest probability each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'argmax' is not defined"
     ]
    }
   ],
   "source": [
    "greedy_decoder(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(DataLoaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dls.train_ds:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = path/'SimpleHTR'/'data'/'test.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tensor(Image.open(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes = [5, 5, 3, 3, 3]\n",
    "channels = [1, 32, 64, 128, 128, 256]\n",
    "strides = pools = [(2,2), (2,2), (2,1), (2,1), (2,1)] \n",
    "paddings = [(2,2), (2,2), (1,1), (1,1), (1,1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def preprocess(img, imgSize, dataAugmentation=False):\n",
    "\t\"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "\n",
    "\t# there are damaged files in IAM dataset - just use black image instead\n",
    "\tif img is None:\n",
    "\t\timg = np.zeros([imgSize[1], imgSize[0]])\n",
    "\n",
    "\t# increase dataset size by applying random stretches to the images\n",
    "\tif dataAugmentation:\n",
    "\t\tstretch = (random.random() - 0.5) # -0.5 .. +0.5\n",
    "\t\twStretched = max(int(img.shape[1] * (1 + stretch)), 1) # random width, but at least 1\n",
    "\t\timg = cv2.resize(img, (wStretched, img.shape[0])) # stretch horizontally by factor 0.5 .. 1.5\n",
    "\t\n",
    "\t# create target image and copy sample image into it\n",
    "\t(wt, ht) = imgSize\n",
    "\t(h, w) = img.shape\n",
    "\tfx = w / wt\n",
    "\tfy = h / ht\n",
    "\tf = max(fx, fy)\n",
    "\tnewSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n",
    "\timg = cv2.resize(img, newSize)\n",
    "\ttarget = np.ones([ht, wt]) * 255\n",
    "\ttarget[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "\t# transpose for TF\n",
    "\timg = cv2.transpose(target)\n",
    "\n",
    "\t# normalize\n",
    "\t(m, s) = cv2.meanStdDev(img)\n",
    "\tm = m[0][0]\n",
    "\ts = s[0][0]\n",
    "\timg = img - m\n",
    "\timg = img / s if s>0 else img\n",
    "\treturn img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_image = cv2.imread(\"../SimpleHTR/data/test.png\", cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_image = preprocess(img=numpy_image, imgSize=(128, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pro_image.shape)\n",
    "show_image(pro_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_tensor = torch.from_numpy(pro_image).float()\n",
    "pro_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pro_tensor.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tensor = processed.unsqueeze(0).unsqueeze(0)\n",
    "final_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        kernel_sizes = [5, 5, 3, 3, 3]\n",
    "        channels = [1, 32, 64, 128, 128, 256]\n",
    "        strides = pools = [(2,2), (2,2), (1,2), (1,2), (1,2)] \n",
    "        paddings = [(2,2), (2,2), (1,1), (1,1), (1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(Module):\n",
    "    def __init__(self, nIn, nH, nOut, bs):\n",
    "        self.rnn = nn.LSTM(nIn, nH, bidirectional=True)\n",
    "        self.h_o = nn.Linear(nH*2, nOut) # LSTM is bidirectional\n",
    "        self.h = [torch.zeros(2, bs, nH) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res, h = self.rnn(x, self.h)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "s = string.ascii_lowercase;s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePaths:\n",
    "    \"filenames and paths to data\"\n",
    "    fnCharList = '../model/charList.txt'\n",
    "    fnAccuracy = '../model/accuracy.txt'\n",
    "    fnTrain = '../data/'\n",
    "    fnInfer = '../data/test.png'\n",
    "    fnCorpus = '../data/corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def preprocess(img, imgSize, dataAugmentation=False):\n",
    "    \"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "    \n",
    "    # there are damaged files in IAM dataset - just use black image instead\n",
    "    if img is None:\n",
    "        img = np.zeros([imgSize[1], imgSize[0]])\n",
    "    \n",
    "    # increase dataset size by applying random stretches to the images\n",
    "    if dataAugmentation:\n",
    "        stretch = (random.random() - 0.5) # -0.5 .. +0.5\n",
    "        wStretched = max(int(img.shape[1] * (1 + stretch)), 1) # random width, but at least 1\n",
    "        img = cv2.resize(img, (wStretched, img.shape[0])) # stretch horizontally by factor 0.5 .. 1.5\n",
    "    \n",
    "    # create target image and copy sample image into it\n",
    "    (wt, ht) = imgSize\n",
    "    (h, w) = img.shape\n",
    "    fx = w / wt\n",
    "    fy = h / ht\n",
    "    f = max(fx, fy)\n",
    "    newSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n",
    "    img = cv2.resize(img, newSize)\n",
    "    target = np.ones([ht, wt]) * 255\n",
    "    target[0:newSize[1], 0:newSize[0]] = img\n",
    "    \n",
    "    # transpose for TF\n",
    "    img = cv2.transpose(target)\n",
    "    \n",
    "    # normalize\n",
    "    (m, s) = cv2.meanStdDev(img)\n",
    "    m = m[0][0]\n",
    "    s = s[0][0]\n",
    "    img = img - m\n",
    "    img = img / s if s>0 else img\n",
    "    return img\n",
    "\n",
    "\n",
    "class Sample:\n",
    "    \"sample from the dataset\"\n",
    "    def __init__(self, gtText, filePath):\n",
    "        self.gtText = gtText\n",
    "        self.filePath = filePath\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    \"batch containing images and ground truth texts\"\n",
    "    def __init__(self, gtTexts, imgs):\n",
    "        self.imgs = torch.from_numpy(np.stack(imgs, axis=0)).float()\n",
    "        self.gtTexts = gtTexts\n",
    "        \n",
    "class DataLoader:\n",
    "    \"loads data which corresponds to IAM format, see: http://www.fki.inf.unibe.ch/databases/iam-handwriting-database\" \n",
    "    \n",
    "    def __init__(self, file_path, batch_size, image_size, max_seq_len):\n",
    "        \"loader for dataset at given location, preprocess images and text according to parameters\"\n",
    "    \n",
    "        assert file_path[-1]=='/'\n",
    "    \n",
    "        self.dataAugmentation = False\n",
    "        self.currIdx = 0\n",
    "        self.batchSize = batch_size\n",
    "        self.imgSize = image_size\n",
    "        self.samples = []\n",
    "    \n",
    "        f=open(file_path+'words.txt')\n",
    "        chars = set()\n",
    "        bad_samples = []\n",
    "        bad_samples_reference = ['a01-117-05-02.png', 'r06-022-03-05.png']\n",
    "        for line in f:\n",
    "            # ignore comment line\n",
    "            if not line or line[0]=='#':\n",
    "                continue\n",
    "            \n",
    "            lineSplit = line.strip().split(' ')\n",
    "            assert len(lineSplit) >= 9\n",
    "            \n",
    "            # filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
    "            fileNameSplit = lineSplit[0].split('-')\n",
    "            fileName = file_path + 'words/' + fileNameSplit[0] + '/' + fileNameSplit[0] + '-' + fileNameSplit[1] + '/' + lineSplit[0] + '.png'\n",
    "    \n",
    "            # GT text are columns starting at 9\n",
    "            gtText = self.truncateLabel(' '.join(lineSplit[8:]), max_seq_len)\n",
    "            chars = chars.union(set(list(gtText)))\n",
    "    \n",
    "            # check if image is not empty\n",
    "            if not os.path.getsize(fileName):\n",
    "                bad_samples.append(lineSplit[0] + '.png')\n",
    "                continue\n",
    "    \n",
    "            # put sample into list\n",
    "            self.samples.append(Sample(gtText, fileName))\n",
    "    \n",
    "        # some images in the IAM dataset are known to be damaged, don't show warning for them\n",
    "        if set(bad_samples) != set(bad_samples_reference):\n",
    "            print(\"Warning, damaged images found:\", bad_samples)\n",
    "            print(\"Damaged images expected:\", bad_samples_reference)\n",
    "    \n",
    "        # split into training and validation set: 95% - 5%\n",
    "        splitIdx = int(0.95 * len(self.samples))\n",
    "        self.trainSamples = self.samples[:splitIdx]\n",
    "        self.validationSamples = self.samples[splitIdx:]\n",
    "    \n",
    "        # put words into lists\n",
    "        self.trainWords = [x.gtText for x in self.trainSamples]\n",
    "        self.validationWords = [x.gtText for x in self.validationSamples]\n",
    "    \n",
    "        # number of randomly chosen samples per epoch for training \n",
    "        self.numTrainSamplesPerEpoch = int(0.95 * len(self.samples)) \n",
    "        \n",
    "        # start with train set\n",
    "        self.trainSet()\n",
    "    \n",
    "        # list of all chars in dataset\n",
    "        self.charList = sorted(list(chars))\n",
    "    \n",
    "    def getBatchSize(self):\n",
    "        return self.batchSize\n",
    "    \n",
    "    def truncateLabel(self, text, maxTextLen):\n",
    "        # ctc_loss can't compute loss if it cannot find a mapping between text label and input \n",
    "        # labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
    "        # If a too-long label is provided, ctc_loss returns an infinite gradient\n",
    "        cost = 0\n",
    "        for i in range(len(text)):\n",
    "            if i != 0 and text[i] == text[i-1]:\n",
    "                cost += 2\n",
    "            else:\n",
    "                cost += 1\n",
    "            if cost > maxTextLen:\n",
    "                return text[:i]\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def trainSet(self):\n",
    "        \"switch to randomly chosen subset of training set\"\n",
    "        self.dataAugmentation = True\n",
    "        self.currIdx = 0\n",
    "        random.shuffle(self.trainSamples)\n",
    "        #self.samples = self.trainSamples[:self.numTrainSamplesPerEpoch]\n",
    "        self.samples = self.trainSamples\n",
    "    \n",
    "    \n",
    "    def validationSet(self):\n",
    "        \"switch to validation set\"\n",
    "        self.dataAugmentation = False\n",
    "        self.currIdx = 0\n",
    "        self.samples = self.validationSamples\n",
    "    \n",
    "    \n",
    "    def getIteratorInfo(self):\n",
    "        \"current batch index and overall number of batches\"\n",
    "        return (self.currIdx // self.batchSize + 1, len(self.samples) // self.batchSize)\n",
    "    \n",
    "    \n",
    "    def hasNext(self):\n",
    "        \"iterator\"\n",
    "        return self.currIdx + self.batchSize <= len(self.samples)\n",
    "        \n",
    "        \n",
    "    def getNext(self):\n",
    "        \"iterator\"\n",
    "        batchRange = range(self.currIdx, self.currIdx + self.batchSize)\n",
    "        gtTexts = [self.samples[i].gtText for i in batchRange]\n",
    "        imgs = [preprocess(cv2.imread(self.samples[i].filePath, cv2.IMREAD_GRAYSCALE), self.imgSize, self.dataAugmentation) for i in batchRange]\n",
    "        self.currIdx += self.batchSize\n",
    "        return self.get_data(Batch(gtTexts, imgs))\n",
    "    \n",
    "    def get_data(self, batch):\n",
    "        targets = []\n",
    "        target_lengths = []\n",
    "        for word in batch.gtTexts:\n",
    "            count = 0\n",
    "            for c in word:\n",
    "                targets.append(dl.charList.index(c))\n",
    "                count += 1\n",
    "            target_lengths.append(count)\n",
    "            assert len(targets) == sum(target_lengths)\n",
    "        return batch.imgs.unsqueeze(1), tensor(*targets), tensor(*target_lengths)\n",
    "    \n",
    "    def one_batch(self):\n",
    "        batchRange = range(0, 64)\n",
    "        gtTexts = [self.samples[i].gtText for i in batchRange]\n",
    "        imgs = [preprocess(cv2.imread(self.samples[i].filePath, cv2.IMREAD_GRAYSCALE), self.imgSize, self.dataAugmentation) for i in batchRange]\n",
    "        return Batch(gtTexts, imgs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "img_size = (128, 32)\n",
    "max_seq_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(FilePaths.fnTrain, bs, img_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = sample_batch.imgs, sample_batch.gtTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(imgs[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(Module):\n",
    "    def __init__(self, vocab_sz, bs):\n",
    "        kernel_sizes = [5, 5, 3, 3, 3]\n",
    "        channels = [1, 32, 64, 128, 128, 256]\n",
    "        strides = pools = [(2,2), (2,2), (1,2), (1,2), (1,2)] \n",
    "        paddings = [(2,2), (2,2), (1,1), (1,1), (1,1)] \n",
    "        layers = []\n",
    "        for i in range(len(strides)):\n",
    "            layers.append(nn.Conv2d(channels[i], channels[i+1], kernel_size=kernel_sizes[i], stride=1, padding=paddings[i]))\n",
    "            layers.append(nn.BatchNorm2d(channels[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(kernel_size=(pools[i][0], pools[i][1]), stride=(strides[i][0], strides[i][1])))\n",
    "        self.cnn = nn.Sequential(*layers)    \n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=256, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True) \n",
    "        self.h_o = nn.Linear(256*2, vocab_sz + 1)\n",
    "        self.h = [torch.zeros(2*2, bs, 256).cuda() for _ in range(2)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x) #N x C x W x H\n",
    "        out = out.squeeze(3)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        res, h = self.rnn(out, self.h)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self):\n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ctc_loss_func(preds, targets, target_lengths):\n",
    "    preds = F.log_softmax(preds, dim=2)\n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    pred_lengths = torch.full(size=(64,), fill_value=32, dtype=torch.long)\n",
    "    loss = nn.CTCLoss(blank=79, reduction='sum')\n",
    "    return loss(preds, targets, pred_lengths, target_lengths)\n",
    "\n",
    "def train_epoch(model, loader, opt, loss_func):\n",
    "    batch_losses = []\n",
    "    while loader.hasNext():\n",
    "        iterInfo = loader.getIteratorInfo()\n",
    "        imgs, targets, target_lengths = loader.getNext()\n",
    "        # Move input data to GPU if it is available\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        # Make predictions\n",
    "        preds = model(imgs)\n",
    "        # Calculate the loss \n",
    "        loss = loss_func(preds, targets, target_lengths)\n",
    "        loss = loss/loader.getBatchSize()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        batch_losses.append(loss.item())\n",
    "        if (iterInfo[0] % 100 == 299):\n",
    "            print(f'Batch {iterInfo[0]}/{iterInfo[1]} loss: {loss}')\n",
    "    return np.array(batch_losses).mean()\n",
    "\n",
    "def validate_epoch(model, dl, opt, loss_func):\n",
    "    batch_losses = []\n",
    "    while dl.hasNext():\n",
    "        imgs, targets, target_lengths = dl.getNext()\n",
    "        # Move input data to GPU if it is available\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        # Make predictions\n",
    "        preds = model(imgs)\n",
    "        # Calculate the loss \n",
    "        loss = loss_func(preds, targets, target_lengths)\n",
    "        loss = loss/dl.getBatchSize()\n",
    "        batch_losses.append(loss.item())\n",
    "    return np.array(batch_losses).mean()\n",
    "\n",
    "            \n",
    "def train_model(model, dl, opt, loss_func, epoch=1):\n",
    "    for i in range(1, epoch + 1):\n",
    "        dl.trainSet()\n",
    "        epoch_loss = train_epoch(model, dl, opt, loss_func)\n",
    "        print(f\"Epoch {i} / {epoch} loss: {epoch_loss}\")\n",
    "        model.reset()\n",
    "        with torch.no_grad():\n",
    "            dl.validationSet()\n",
    "            validation_loss = validate_epoch(model, dl, opt, loss_func)\n",
    "            print(f\"Epoch {i}/{epoch} validation loss: {validation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN(len(dl.charList), 64).to(device)\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, dl, opt, ctc_loss_func, epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"model-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.trainSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dl.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = batch.imgs, batch.gtTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = str().join(dl.charList)\n",
    "wordChars = open('../data/wordCharList.txt').read().splitlines()[0]\n",
    "corpus = open('../data/corpus.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = str().join(dl.charList)\n",
    "wordChars = open('../data/wordCharList.txt').read().splitlines()[0]\n",
    "corpus = open('../data/corpus.txt').read()\n",
    "# init decoder object\n",
    "# this step usually only has to be done once in a program\n",
    "beamWidth = 25\n",
    "lmType = 'NGrams'\n",
    "lmSmoothing = 0.01\n",
    "wbs = WordBeamSearch(beamWidth, lmType, lmSmoothing, corpus, chars, wordChars)\n",
    "\n",
    "# feed NumPy array of shape TxBxC to decoder, which returns list of decoded texts\n",
    "#res = wbs.compute(feedMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(imgs.unsqueeze(1).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedMat = preds.permute(1,0,2).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedMat = feedMat.to(torch.device('cpu')).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wbs.compute(feedMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(pred):\n",
    "    blank = 79\n",
    "    s = ''\n",
    "    for label in pred:\n",
    "        if label == blank:\n",
    "            break\n",
    "        s += chars[label]  #\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(imgs[6].squeeze()), labels[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(res[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank = 79\n",
    "s = ''\n",
    "for label in res[10]:\n",
    "    if label == blank:\n",
    "        break\n",
    "    s += chars[label]  #\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(imgs[10][0]), batch.gtTexts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "target_lengths = []\n",
    "for word in ground_truth:\n",
    "    count = 0\n",
    "    for c in word:\n",
    "        targets.append(dl.charList.index(c))\n",
    "        count += 1\n",
    "    target_lengths.append(count)\n",
    "targets = tensor(*targets)\n",
    "target_lengths = tensor(*target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(preds, targets, target_lengths):\n",
    "    preds = F.log_softmax(preds, dim=2)\n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    pred_lengths = torch.full(size=(64,), fill_value=32, dtype=torch.long)\n",
    "    loss = nn.CTCLoss(blank=79)\n",
    "    return loss(preds, targets, pred_lengths, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    batchSize = 64\n",
    "    imgSize = (128, 32)\n",
    "    maxTextLen = 32\n",
    "    \n",
    "@Transform\n",
    "def img_tfm(x: PILImage):\n",
    "    data = np.array(x)\n",
    "    result = torch.from_numpy(preprocess(data, (512, 32), True)).float()\n",
    "    return result.unsqueeze(0)\n",
    "\n",
    "def get_x(r): return r['fnames']\n",
    "def get_y(r): return r['encoded_labels']\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock(cls=PILImageBW), None),\n",
    "    get_x = get_x, \n",
    "    get_y = get_y,\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    item_tfms=img_tfm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, imgSize, dataAugmentation=False):\n",
    "\t\"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "\n",
    "\t# there are damaged files in IAM dataset - just use black image instead\n",
    "\tif img is None:\n",
    "\t\timg = np.zeros([imgSize[1], imgSize[0]])\n",
    "\n",
    "\t# increase dataset size by applying random stretches to the images\n",
    "\tif dataAugmentation:\n",
    "\t\tstretch = (random.random() - 0.5) # -0.5 .. +0.5\n",
    "\t\twStretched = max(int(img.shape[1] * (1 + stretch)), 1) # random width, but at least 1\n",
    "\t\timg = cv2.resize(img, (wStretched, img.shape[0])) # stretch horizontally by factor 0.5 .. 1.5\n",
    "\t\n",
    "\t# create target image and copy sample image into it\n",
    "\t(wt, ht) = imgSize\n",
    "\t(h, w) = img.shape\n",
    "\tfx = w / wt\n",
    "\tfy = h / ht\n",
    "\tf = max(fx, fy)\n",
    "\tnewSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n",
    "\timg = cv2.resize(img, newSize)\n",
    "\ttarget = np.ones([ht, wt]) * 255\n",
    "\ttarget[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "\t# transpose for TF\n",
    "\timg = cv2.transpose(target)\n",
    "\n",
    "\t# normalize\n",
    "\t(m, s) = cv2.meanStdDev(img)\n",
    "\tm = m[0][0]\n",
    "\ts = s[0][0]\n",
    "\timg = img - m\n",
    "\timg = img / s if s>0 else img\n",
    "\treturn img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(preprocess(np.array(Image.open(files[0])), (512, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = preprocess(np.array(Image.open(files[0])), (512, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def preprocess(img, imgSize, dataAugmentation=False):\n",
    "\t\"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "\n",
    "\t# there are damaged files in IAM dataset - just use black image instead\n",
    "\tif img is None:\n",
    "\t\timg = np.zeros([imgSize[1], imgSize[0]])\n",
    "\n",
    "\t# increase dataset size by applying random stretches to the images\n",
    "\tif dataAugmentation:\n",
    "\t\tstretch = (random.random() - 0.5) # -0.5 .. +0.5\n",
    "\t\twStretched = max(int(img.shape[1] * (1 + stretch)), 1) # random width, but at least 1\n",
    "\t\timg = cv2.resize(img, (wStretched, img.shape[0])) # stretch horizontally by factor 0.5 .. 1.5\n",
    "\t\n",
    "\t# create target image and copy sample image into it\n",
    "\t(wt, ht) = imgSize\n",
    "\t(h, w) = img.shape\n",
    "\tfx = w / wt\n",
    "\tfy = h / ht\n",
    "\tf = max(fx, fy)\n",
    "\tnewSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n",
    "\timg = cv2.resize(img, newSize)\n",
    "\ttarget = np.ones([ht, wt]) * 255\n",
    "\ttarget[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "\t# transpose for TF\n",
    "\timg = cv2.transpose(target)\n",
    "\n",
    "\t# normalize\n",
    "\t(m, s) = cv2.meanStdDev(img)\n",
    "\tm = m[0][0]\n",
    "\ts = s[0][0]\n",
    "\timg = img - m\n",
    "\timg = img / s if s>0 else img\n",
    "\treturn img\n",
    "\n",
    "class Sample:\n",
    "\t\"sample from the dataset\"\n",
    "\tdef __init__(self, gtText, filePath):\n",
    "\t\tself.gtText = gtText\n",
    "\t\tself.filePath = filePath\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    \"batch containing images and ground truth texts\"\n",
    "    def __init__(self, gtTexts, imgs):\n",
    "        self.imgs = torch.from_numpy(np.stack(imgs, axis=0)).float()\n",
    "        self.gtTexts = gtTexts\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "\t\"loads data which corresponds to IAM format, see: http://www.fki.inf.unibe.ch/databases/iam-handwriting-database\" \n",
    "\n",
    "\tdef __init__(self, filePath, batchSize, imgSize, maxTextLen):\n",
    "\t\t\"loader for dataset at given location, preprocess images and text according to parameters\"\n",
    "\n",
    "\t\tassert filePath[-1]=='/'\n",
    "\n",
    "\t\tself.dataAugmentation = False\n",
    "\t\tself.currIdx = 0\n",
    "\t\tself.batchSize = batchSize\n",
    "\t\tself.imgSize = imgSize\n",
    "\t\tself.samples = []\n",
    "\t\n",
    "\t\tf=open(filePath+'words.txt')\n",
    "\t\tchars = set()\n",
    "\t\tbad_samples = []\n",
    "\t\tbad_samples_reference = ['a01-117-05-02.png', 'r06-022-03-05.png']\n",
    "\t\tfor line in f:\n",
    "\t\t\t# ignore comment line\n",
    "\t\t\tif not line or line[0]=='#':\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\tlineSplit = line.strip().split(' ')\n",
    "\t\t\tassert len(lineSplit) >= 9\n",
    "\t\t\t\n",
    "\t\t\t# filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
    "\t\t\tfileNameSplit = lineSplit[0].split('-')\n",
    "\t\t\tfileName = filePath + 'words/' + fileNameSplit[0] + '/' + fileNameSplit[0] + '-' + fileNameSplit[1] + '/' + lineSplit[0] + '.png'\n",
    "\n",
    "\t\t\t# GT text are columns starting at 9\n",
    "\t\t\tgtText = self.truncateLabel(' '.join(lineSplit[8:]), maxTextLen)\n",
    "\t\t\tchars = chars.union(set(list(gtText)))\n",
    "\n",
    "\t\t\t# check if image is not empty\n",
    "\t\t\tif not os.path.getsize(fileName):\n",
    "\t\t\t\tbad_samples.append(lineSplit[0] + '.png')\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# put sample into list\n",
    "\t\t\tself.samples.append(Sample(gtText, fileName))\n",
    "\n",
    "\t\t# some images in the IAM dataset are known to be damaged, don't show warning for them\n",
    "\t\tif set(bad_samples) != set(bad_samples_reference):\n",
    "\t\t\tprint(\"Warning, damaged images found:\", bad_samples)\n",
    "\t\t\tprint(\"Damaged images expected:\", bad_samples_reference)\n",
    "\n",
    "\t\t# split into training and validation set: 95% - 5%\n",
    "\t\tsplitIdx = int(0.95 * len(self.samples))\n",
    "\t\tself.trainSamples = self.samples[:splitIdx]\n",
    "\t\tself.validationSamples = self.samples[splitIdx:]\n",
    "\n",
    "\t\t# put words into lists\n",
    "\t\tself.trainWords = [x.gtText for x in self.trainSamples]\n",
    "\t\tself.validationWords = [x.gtText for x in self.validationSamples]\n",
    "\n",
    "\t\t# number of randomly chosen samples per epoch for training \n",
    "\t\tself.numTrainSamplesPerEpoch = 25000 \n",
    "\t\t\n",
    "\t\t# start with train set\n",
    "\t\tself.trainSet()\n",
    "\n",
    "\t\t# list of all chars in dataset\n",
    "\t\tself.charList = sorted(list(chars))\n",
    "\n",
    "\tdef getBatchSize(self):\n",
    "\t\treturn self.batchSize\n",
    "\n",
    "\tdef truncateLabel(self, text, maxTextLen):\n",
    "\t\t# ctc_loss can't compute loss if it cannot find a mapping between text label and input \n",
    "\t\t# labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
    "\t\t# If a too-long label is provided, ctc_loss returns an infinite gradient\n",
    "\t\tcost = 0\n",
    "\t\tfor i in range(len(text)):\n",
    "\t\t\tif i != 0 and text[i] == text[i-1]:\n",
    "\t\t\t\tcost += 2\n",
    "\t\t\telse:\n",
    "\t\t\t\tcost += 1\n",
    "\t\t\tif cost > maxTextLen:\n",
    "\t\t\t\treturn text[:i]\n",
    "\t\treturn text\n",
    "\n",
    "\n",
    "\tdef trainSet(self):\n",
    "\t\t\"switch to randomly chosen subset of training set\"\n",
    "\t\tself.dataAugmentation = True\n",
    "\t\tself.currIdx = 0\n",
    "\t\trandom.shuffle(self.trainSamples)\n",
    "\t\tself.samples = self.trainSamples[:self.numTrainSamplesPerEpoch]\n",
    "\n",
    "\t\n",
    "\tdef validationSet(self):\n",
    "\t\t\"switch to validation set\"\n",
    "\t\tself.dataAugmentation = False\n",
    "\t\tself.currIdx = 0\n",
    "\t\tself.samples = self.validationSamples\n",
    "\n",
    "\n",
    "\tdef getIteratorInfo(self):\n",
    "\t\t\"current batch index and overall number of batches\"\n",
    "\t\treturn (self.currIdx // self.batchSize + 1, len(self.samples) // self.batchSize)\n",
    "\n",
    "\n",
    "\tdef hasNext(self):\n",
    "\t\t\"iterator\"\n",
    "\t\treturn self.currIdx + self.batchSize <= len(self.samples)\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef getNext(self):\n",
    "\t\t\"iterator\"\n",
    "\t\tbatchRange = range(self.currIdx, self.currIdx + self.batchSize)\n",
    "\t\tgtTexts = [self.samples[i].gtText for i in batchRange]\n",
    "\t\timgs = [preprocess(cv2.imread(self.samples[i].filePath, cv2.IMREAD_GRAYSCALE), self.imgSize, self.dataAugmentation) for i in batchRange]\n",
    "\t\tself.currIdx += self.batchSize\n",
    "\t\treturn Batch(gtTexts, imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_image_files(path):\n",
    "    bad_samples = []\n",
    "    bad_sample_references = ['a01-117-05-02.png', 'r06-022-03-05.png']\n",
    "    finals = L()\n",
    "    files = get_image_files(path)\n",
    "    for f in files:\n",
    "        # check if image is empty\n",
    "        if not os.path.getsize(f):\n",
    "            bad_samples.append(str(f).split('/')[-1])\n",
    "        else:\n",
    "            finals.append(f)\n",
    "            \n",
    "    if set(bad_samples) != set(bad_sample_references):\n",
    "        print(\"Warning, damaged images found:\", bad_samples)\n",
    "        print(\"Damaged images expected:\", bad_sample_references)\n",
    "        \n",
    "    return finals\n",
    "\n",
    "def truncateLabel(text, maxTextLen):\n",
    "    # ctc_loss can't compute loss if it cannot find a mapping between text label and input \n",
    "    # labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
    "    # If a too-long label is provided, ctc_loss returns an infinite gradient\n",
    "    cost = 0\n",
    "    for i in range(len(text)):\n",
    "        if i != 0 and text[i] == text[i-1]:\n",
    "            cost += 2\n",
    "        else:\n",
    "            cost += 1\n",
    "        if cost > maxTextLen:\n",
    "            return text[:i]\n",
    "    return text\n",
    "\n",
    "def my_get_label(fname):\n",
    "    path = Path(\"../data/words.txt\")\n",
    "    maxTextLen = 32\n",
    "    for idx, line in enumerate(f):\n",
    "        if (line[0] != '#'):\n",
    "            print(idx, truncateLabel(' '.join(line.strip().split(\" \")[8:]), maxTextLen))\n",
    "    return \n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items = my_get_image_files,\n",
    "    get_y = my_get_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../data/words.txt\")\n",
    "f = open(path)\n",
    "file_path = Path(\"../data/words\")\n",
    "maxTextLen = 32\n",
    "fnames = L()\n",
    "labels = L()\n",
    "chars = set()\n",
    "bad_samples = []\n",
    "bad_sample_references = ['a01-117-05-02.png', 'r06-022-03-05.png']\n",
    "for line in f:\n",
    "    if line[0] != \"#\":\n",
    "        words = line.strip().split(\" \")\n",
    "        assert len(words) >= 9\n",
    "        \n",
    "        # Get ground truth text\n",
    "        label = truncateLabel(' '.join(words[8:]), maxTextLen)\n",
    "        chars = chars.union(set(list(label)))\n",
    "        # Get file name\n",
    "        file_name_split = words[0].split('-')\n",
    "        file_name = file_path/file_name_split[0]/ \\\n",
    "            (file_name_split[0] + \"-\" + file_name_split[1])/ \\\n",
    "            (words[0]+\".png\")\n",
    "        \n",
    "        # check if image is empty\n",
    "        if not os.path.getsize(file_name):\n",
    "            bad_samples.append(str(file_name).split('/')[-1])\n",
    "        else:\n",
    "            fnames.append(file_name)\n",
    "            labels.append(label)\n",
    "            \n",
    "assert len(labels) == len(fnames)\n",
    "if set(bad_samples) != set(bad_sample_references):\n",
    "    print(\"Warning, damaged images found:\", bad_samples)\n",
    "    print(\"Damaged images expected:\", bad_sample_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"fname\": fnames,\n",
    "    \"label\": labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def preprocess(img, imgSize, dataAugmentation=False):\n",
    "\t\"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "\n",
    "\t# there are damaged files in IAM dataset - just use black image instead\n",
    "\tif img is None:\n",
    "\t\timg = np.zeros([imgSize[1], imgSize[0]])\n",
    "\n",
    "\t# increase dataset size by applying random stretches to the images\n",
    "\tif dataAugmentation:\n",
    "\t\tstretch = (random.random() - 0.5) # -0.5 .. +0.5\n",
    "\t\twStretched = max(int(img.shape[1] * (1 + stretch)), 1) # random width, but at least 1\n",
    "\t\timg = cv2.resize(img, (wStretched, img.shape[0])) # stretch horizontally by factor 0.5 .. 1.5\n",
    "\t\n",
    "\t# create target image and copy sample image into it\n",
    "\t(wt, ht) = imgSize\n",
    "\t(h, w) = img.shape\n",
    "\tfx = w / wt\n",
    "\tfy = h / ht\n",
    "\tf = max(fx, fy)\n",
    "\tnewSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1)) # scale according to f (result at least 1 and at most wt or ht)\n",
    "\timg = cv2.resize(img, newSize)\n",
    "\ttarget = np.ones([ht, wt]) * 255\n",
    "\ttarget[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "\t# transpose for TF\n",
    "\timg = cv2.transpose(target)\n",
    "\n",
    "\t# normalize\n",
    "\t(m, s) = cv2.meanStdDev(img)\n",
    "\tm = m[0][0]\n",
    "\ts = s[0][0]\n",
    "\timg = img - m\n",
    "\timg = img / s if s>0 else img\n",
    "\treturn img\n",
    "\n",
    "class Sample:\n",
    "\t\"sample from the dataset\"\n",
    "\tdef __init__(self, gtText, filePath):\n",
    "\t\tself.gtText = gtText\n",
    "\t\tself.filePath = filePath\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    \"batch containing images and ground truth texts\"\n",
    "    def __init__(self, gtTexts, imgs):\n",
    "        self.imgs = torch.from_numpy(np.stack(imgs, axis=0)).float()\n",
    "        self.gtTexts = gtTexts\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "\t\"loads data which corresponds to IAM format, see: http://www.fki.inf.unibe.ch/databases/iam-handwriting-database\" \n",
    "\n",
    "\tdef __init__(self, filePath, batchSize, imgSize, maxTextLen):\n",
    "\t\t\"loader for dataset at given location, preprocess images and text according to parameters\"\n",
    "\n",
    "\t\tassert filePath[-1]=='/'\n",
    "\n",
    "\t\tself.dataAugmentation = False\n",
    "\t\tself.currIdx = 0\n",
    "\t\tself.batchSize = batchSize\n",
    "\t\tself.imgSize = imgSize\n",
    "\t\tself.samples = []\n",
    "\t\n",
    "\t\tf=open(filePath+'words.txt')\n",
    "\t\tchars = set()\n",
    "\t\tbad_samples = []\n",
    "\t\tbad_samples_reference = ['a01-117-05-02.png', 'r06-022-03-05.png']\n",
    "\t\tfor line in f:\n",
    "\t\t\t# ignore comment line\n",
    "\t\t\tif not line or line[0]=='#':\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\tlineSplit = line.strip().split(' ')\n",
    "\t\t\tassert len(lineSplit) >= 9\n",
    "\t\t\t\n",
    "\t\t\t# filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
    "\t\t\tfileNameSplit = lineSplit[0].split('-')\n",
    "\t\t\tfileName = filePath + 'words/' + fileNameSplit[0] + '/' + fileNameSplit[0] + '-' + fileNameSplit[1] + '/' + lineSplit[0] + '.png'\n",
    "\n",
    "\t\t\t# GT text are columns starting at 9\n",
    "\t\t\tgtText = self.truncateLabel(' '.join(lineSplit[8:]), maxTextLen)\n",
    "\t\t\tchars = chars.union(set(list(gtText)))\n",
    "\n",
    "\t\t\t# check if image is not empty\n",
    "\t\t\tif not os.path.getsize(fileName):\n",
    "\t\t\t\tbad_samples.append(lineSplit[0] + '.png')\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# put sample into list\n",
    "\t\t\tself.samples.append(Sample(gtText, fileName))\n",
    "\n",
    "\t\t# some images in the IAM dataset are known to be damaged, don't show warning for them\n",
    "\t\tif set(bad_samples) != set(bad_samples_reference):\n",
    "\t\t\tprint(\"Warning, damaged images found:\", bad_samples)\n",
    "\t\t\tprint(\"Damaged images expected:\", bad_samples_reference)\n",
    "\n",
    "\t\t# split into training and validation set: 95% - 5%\n",
    "\t\tsplitIdx = int(0.95 * len(self.samples))\n",
    "\t\tself.trainSamples = self.samples[:splitIdx]\n",
    "\t\tself.validationSamples = self.samples[splitIdx:]\n",
    "\n",
    "\t\t# put words into lists\n",
    "\t\tself.trainWords = [x.gtText for x in self.trainSamples]\n",
    "\t\tself.validationWords = [x.gtText for x in self.validationSamples]\n",
    "\n",
    "\t\t# number of randomly chosen samples per epoch for training \n",
    "\t\tself.numTrainSamplesPerEpoch = 25000 \n",
    "\t\t\n",
    "\t\t# start with train set\n",
    "\t\tself.trainSet()\n",
    "\n",
    "\t\t# list of all chars in dataset\n",
    "\t\tself.charList = sorted(list(chars))\n",
    "\n",
    "\tdef getBatchSize(self):\n",
    "\t\treturn self.batchSize\n",
    "\n",
    "\tdef truncateLabel(self, text, maxTextLen):\n",
    "\t\t# ctc_loss can't compute loss if it cannot find a mapping between text label and input \n",
    "\t\t# labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
    "\t\t# If a too-long label is provided, ctc_loss returns an infinite gradient\n",
    "\t\tcost = 0\n",
    "\t\tfor i in range(len(text)):\n",
    "\t\t\tif i != 0 and text[i] == text[i-1]:\n",
    "\t\t\t\tcost += 2\n",
    "\t\t\telse:\n",
    "\t\t\t\tcost += 1\n",
    "\t\t\tif cost > maxTextLen:\n",
    "\t\t\t\treturn text[:i]\n",
    "\t\treturn text\n",
    "\n",
    "\n",
    "\tdef trainSet(self):\n",
    "\t\t\"switch to randomly chosen subset of training set\"\n",
    "\t\tself.dataAugmentation = True\n",
    "\t\tself.currIdx = 0\n",
    "\t\trandom.shuffle(self.trainSamples)\n",
    "\t\tself.samples = self.trainSamples[:self.numTrainSamplesPerEpoch]\n",
    "\n",
    "\t\n",
    "\tdef validationSet(self):\n",
    "\t\t\"switch to validation set\"\n",
    "\t\tself.dataAugmentation = False\n",
    "\t\tself.currIdx = 0\n",
    "\t\tself.samples = self.validationSamples\n",
    "\n",
    "\n",
    "\tdef getIteratorInfo(self):\n",
    "\t\t\"current batch index and overall number of batches\"\n",
    "\t\treturn (self.currIdx // self.batchSize + 1, len(self.samples) // self.batchSize)\n",
    "\n",
    "\n",
    "\tdef hasNext(self):\n",
    "\t\t\"iterator\"\n",
    "\t\treturn self.currIdx + self.batchSize <= len(self.samples)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\"iterator\"\n",
    "\t\tbatchRange = range(self.currIdx, self.currIdx + self.batchSize)\n",
    "\t\tgtTexts = [self.samples[i].gtText for i in batchRange]\n",
    "\t\timgs = [preprocess(cv2.imread(self.samples[i].filePath, cv2.IMREAD_GRAYSCALE), self.imgSize, self.dataAugmentation) for i in batchRange]\n",
    "\t\tself.currIdx += self.batchSize\n",
    "\t\treturn Batch(gtTexts, imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Model:\n",
    "    batchSize = 64\n",
    "    imgSize = (128, 32)\n",
    "    maxTextLen = 32\n",
    "    \n",
    "@Transform\n",
    "def img_tfm(x: PILImage):\n",
    "    data = np.array(x)\n",
    "    result = torch.from_numpy(preprocess(data, (128, 32), True)).float()\n",
    "    return result.unsqueeze(0)\n",
    "def get_x(r): return r['fname']\n",
    "def get_y(r): return r['label']\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock(cls=PILImageBW), None),\n",
    "    get_x = get_x, \n",
    "    get_y = get_y,\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    item_tfms=img_tfm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(r): return r['fname']\n",
    "def get_y(r): return r['label']\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock(cls=PILImageBW), None),\n",
    "    get_x = get_x, \n",
    "    get_y = get_y,\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    item_tfms=img_tfm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(df, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charList = sorted(list(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = str().join(charList)\n",
    "wordChars = open('../data/wordCharList.txt').read().splitlines()[0]\n",
    "corpus = open('../data/corpus.txt').read()\n",
    "# init decoder object\n",
    "# this step usually only has to be done once in a program\n",
    "beamWidth = 25\n",
    "lmType = 'NGrams'\n",
    "lmSmoothing = 0.01\n",
    "wbs = WordBeamSearch(beamWidth, lmType, lmSmoothing, corpus, chars, wordChars)\n",
    "\n",
    "# feed NumPy array of shape TxBxC to decoder, which returns list of decoded texts\n",
    "#res = wbs.compute(feedMat)\n",
    "\n",
    "def ctc_loss_func(preds, targets, target_lengths, bs=64, seqLen=32):\n",
    "    preds = F.log_softmax(preds, dim=2)\n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    pred_lengths = torch.full(size=(bs,), fill_value=seqLen, dtype=torch.long)\n",
    "    loss = nn.CTCLoss(blank=79, reduction='sum')\n",
    "    return loss(preds, targets, pred_lengths, target_lengths)\n",
    "\n",
    "def get_targets(text):\n",
    "    targets = []\n",
    "    target_lengths = []\n",
    "    for word in text:\n",
    "        count = 0\n",
    "        for c in word:\n",
    "            targets.append(charList.index(c))\n",
    "            count += 1\n",
    "        target_lengths.append(count)\n",
    "        assert len(targets) == sum(target_lengths)\n",
    "    return tensor(*targets).cuda(), tensor(*target_lengths).cuda()\n",
    "\n",
    "def calc_grad(xb, yb, model, bs=64):\n",
    "    targets, target_lengths = get_targets(yb) \n",
    "    preds = model(xb)\n",
    "    loss = ctc_loss_func(preds, targets, target_lengths)\n",
    "    loss = loss/bs\n",
    "    loss.backward()\n",
    "    return loss\n",
    "    \n",
    "def train_epoch(dl, model, opt):\n",
    "    i = 1\n",
    "    length = len(dl)\n",
    "    for xb, yb in dl:\n",
    "        loss = calc_grad(xb, yb, model)\n",
    "        print(f'batch: {i}/{length} loss: {loss}')\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        i+=1\n",
    "\n",
    "def decode(pred):\n",
    "    blank = 79\n",
    "    s = ''\n",
    "    for label in pred:\n",
    "        if label == blank:\n",
    "            break\n",
    "        s += chars[label]  #\n",
    "    print(s)\n",
    "    \n",
    "def batch_accuracy(xb, yb):\n",
    "    feedMat = xb.permute(1,0,2).detach().numpy()\n",
    "    preds = wbs.compute(feedMat)\n",
    "    for pred in preds:\n",
    "        text = decode(pred)\n",
    "        gtext = decode(yb)\n",
    "        if (text == gtext[:len(text)]): count+=1;\n",
    "    return count / len(preds)\n",
    "    \n",
    "\n",
    "def validate_epoch(dl, model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb, yb in dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n",
    "    \n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from time import sleep\n",
    "def train_model(dls, model, opt, epochs=1):\n",
    "    train_dl = dls.train\n",
    "    valid_dl = dls.valid\n",
    "    train_epoch(train_dl, model, opt)\n",
    "    print(f'Epoch: {i}/{epochs} Loss: {validate_epoch(valid_dl, model)}')\n",
    "    model.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "#     print(\"Running on the GPU\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(Module):\n",
    "    def __init__(self, vocab_sz, bs):\n",
    "        kernel_sizes = [5, 5, 3, 3, 3]\n",
    "        channels = [1, 32, 64, 128, 128, 256]\n",
    "        strides = pools = [(2,2), (2,2), (1,2), (1,2), (1,2)] \n",
    "        paddings = [(2,2), (2,2), (1,1), (1,1), (1,1)] \n",
    "        layers = []\n",
    "        for i in range(len(strides)):\n",
    "            layers.append(nn.Conv2d(channels[i], channels[i+1], kernel_size=kernel_sizes[i], stride=1, padding=paddings[i]))\n",
    "            layers.append(nn.BatchNorm2d(channels[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(kernel_size=(pools[i][0], pools[i][1]), stride=(strides[i][0], strides[i][1])))\n",
    "        self.cnn = nn.Sequential(*layers)    \n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=256, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True) \n",
    "        self.h_o = nn.Linear(256*2, vocab_sz + 1)\n",
    "        self.h = [torch.zeros(2*2, bs, 256) for _ in range(2)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x) #N x C x W x H\n",
    "        out = out.squeeze(3)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        res, h = self.rnn(out, self.h)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self):\n",
    "        for h in self.h: h.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN(len(charList), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if(torch.cuda.is_available()):\n",
    "#     print(\"Using GPU\")\n",
    "#     dls.cuda()# rnn is your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "dls = dls.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(dls, model, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
